{"cells":[{"cell_type":"markdown","metadata":{"id":"rToK0Tku8PPn"},"source":["## Makemore: becoming a backprop ninja"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"8sFElPqq8PPp","executionInfo":{"status":"ok","timestamp":1703449572972,"user_tz":300,"elapsed":139,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}}},"outputs":[],"source":["# there no change change in the first several cells from last lecture"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"ChBbac4y8PPq","executionInfo":{"status":"ok","timestamp":1703449573201,"user_tz":300,"elapsed":2,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt # for making figures\n","%matplotlib inline"]},{"cell_type":"code","source":["# download the names.txt file from github\n","!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"],"metadata":{"id":"x6GhEWW18aCS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703449573366,"user_tz":300,"elapsed":167,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"eadedbe9-f992-48d3-8ee0-cd8f0e482024"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-12-24 20:26:13--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 228145 (223K) [text/plain]\n","Saving to: ‘names.txt.1’\n","\n","\rnames.txt.1           0%[                    ]       0  --.-KB/s               \rnames.txt.1         100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n","\n","2023-12-24 20:26:13 (8.42 MB/s) - ‘names.txt.1’ saved [228145/228145]\n","\n"]}]},{"cell_type":"code","execution_count":67,"metadata":{"id":"klmu3ZG08PPr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703449573367,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"1f9337c8-3bd0-4f85-d15a-7c72f83ea42f"},"outputs":[{"output_type":"stream","name":"stdout","text":["32033\n","15\n","['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"]}],"source":["# read in all the words\n","words = open('names.txt', 'r').read().splitlines()\n","print(len(words))\n","print(max(len(w) for w in words))\n","print(words[:8])"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"BCQomLE_8PPs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703449573367,"user_tz":300,"elapsed":2,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"fbd335d1-ffb2-40fa-81b8-d3266819f7ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n","27\n"]}],"source":["# build the vocabulary of characters and mappings to/from integers\n","chars = sorted(list(set(''.join(words))))\n","stoi = {s:i+1 for i,s in enumerate(chars)}\n","stoi['.'] = 0\n","itos = {i:s for s,i in stoi.items()}\n","vocab_size = len(itos)\n","print(itos)\n","print(vocab_size)"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"V_zt2QHr8PPs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703449574207,"user_tz":300,"elapsed":842,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"5e39acb9-e9ea-421d-92d6-8a556773f5c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([182625, 3]) torch.Size([182625])\n","torch.Size([22655, 3]) torch.Size([22655])\n","torch.Size([22866, 3]) torch.Size([22866])\n"]}],"source":["# build the dataset\n","block_size = 3 # context length: how many characters do we take to predict the next one?\n","\n","def build_dataset(words):\n","  X, Y = [], []\n","\n","  for w in words:\n","    context = [0] * block_size\n","    for ch in w + '.':\n","      ix = stoi[ch]\n","      X.append(context)\n","      Y.append(ix)\n","      context = context[1:] + [ix] # crop and append\n","\n","  X = torch.tensor(X)\n","  Y = torch.tensor(Y)\n","  print(X.shape, Y.shape)\n","  return X, Y\n","\n","import random\n","random.seed(42)\n","random.shuffle(words)\n","n1 = int(0.8*len(words))\n","n2 = int(0.9*len(words))\n","\n","Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n","Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n","Xte,  Yte  = build_dataset(words[n2:])     # 10%"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"eg20-vsg8PPt","executionInfo":{"status":"ok","timestamp":1703449574207,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}}},"outputs":[],"source":["# ok biolerplate done, now we get to the action:"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"MJPU8HT08PPu","executionInfo":{"status":"ok","timestamp":1703449574207,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}}},"outputs":[],"source":["# utility function we will use later when comparing manual gradients to PyTorch gradients\n","def cmp(s, dt, t):\n","  ex = torch.all(dt == t.grad).item()\n","  app = torch.allclose(dt, t.grad)\n","  maxdiff = (dt - t.grad).abs().max().item()\n","  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"ZlFLjQyT8PPu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703449574207,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"1f70a087-b8d7-4c73-9b37-41243bc147fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["4137\n"]}],"source":["n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","# Note: I am initializating many of these parameters in non-standard ways\n","# because sometimes initializating with e.g. all zeros could mask an incorrect\n","# implementation of the backward pass.\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"QY-y96Y48PPv","executionInfo":{"status":"ok","timestamp":1703449574207,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}}},"outputs":[],"source":["batch_size = 32\n","n = batch_size # a shorter variable also, for convenience\n","# construct a minibatch\n","ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"8ofj1s6d8PPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703449574208,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"315a1b07-9c53-4729-aade-c726e08ea18e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.3512, grad_fn=<NegBackward0>)"]},"metadata":{},"execution_count":74}],"source":["# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n","\n","emb = C[Xb] # embed the characters into vectors\n","embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","# Linear layer 1\n","hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n","# BatchNorm layer\n","bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","bndiff = hprebn - bnmeani\n","bndiff2 = bndiff**2\n","bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","bnvar_inv = (bnvar + 1e-5)**-0.5\n","bnraw = bndiff * bnvar_inv\n","hpreact = bngain * bnraw + bnbias\n","# Non-linearity\n","h = torch.tanh(hpreact) # hidden layer\n","# Linear layer 2\n","logits = h @ W2 + b2 # output layer\n","# cross entropy loss (same as F.cross_entropy(logits, Yb))\n","logit_maxes = logits.max(1, keepdim=True).values\n","norm_logits = logits - logit_maxes # subtract max for numerical stability\n","counts = norm_logits.exp()\n","counts_sum = counts.sum(1, keepdims=True)\n","counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","probs = counts * counts_sum_inv\n","logprobs = probs.log()\n","loss = -logprobs[range(n), Yb].mean()\n","\n","# PyTorch backward pass\n","for p in parameters:\n","  p.grad = None\n","for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n","          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n","         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n","         embcat, emb]:\n","  t.retain_grad()\n","loss.backward()\n","loss"]},{"cell_type":"markdown","source":["## Calculating Our Backward Pass\n","We're going to go through each step in calculating the loss, one by one, backwards. This will be our process of manually performing a backword pass through our neural network."],"metadata":{"id":"6nENzhynkbwz"}},{"cell_type":"markdown","source":["### `dlogprobs`\n","\n"],"metadata":{"id":"dSAeS4EIlWzM"}},{"cell_type":"markdown","source":["Let's start with getting the derivative of `logprobs` with respect to `loss`. This is done in the line:\n","\n","```python\n","loss = -logprobs[range(n), Yb].mean()\n","```\n","\n","First, remember the chain rule, which says:\n","\n","$\\frac{d}{dx}f(g(x)) = f'(g(x)) * g'(x)$"],"metadata":{"id":"9lOTguvHwdVf"}},{"cell_type":"code","source":["# Here's the values used to calculate loss (or the operations on logprobs)\n","Yb, logprobs[range(n), Yb]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYFcAdORse69","executionInfo":{"status":"ok","timestamp":1703449574208,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"86df11b8-54de-48ad-eb9e-549cdd259069"},"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n","         26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18]),\n"," tensor([-4.0162, -3.0867, -3.5782, -3.2973, -4.1477, -3.4795, -3.3522, -4.0521,\n","         -3.1649, -4.2128, -3.1414, -1.6849, -2.8205, -2.8727, -3.0433, -3.1802,\n","         -3.7319, -2.9491, -3.5728, -3.4428, -2.8714, -2.9693, -4.2792, -3.9525,\n","         -3.5632, -2.9193, -3.1328, -3.9430, -2.7124, -3.6829, -3.2276, -3.1591],\n","        grad_fn=<IndexBackward0>))"]},"metadata":{},"execution_count":75}]},{"cell_type":"markdown","source":["Here it's useful to think about calculating loss in an example. Let's say we're summing across 3 rows:\n","\n","$loss = \\frac{-1}{3}(a + b + c)$\n","\n","The partial derivative, $\\frac{dloss}{da}$, is $\\frac{-1}{3}$. In our larger case, it's $\\frac{1}{27}$ because we're getting the mean of 27 items (next letter predictions).\n","\n","However, we're not using all 27 items in our loss, so the rest of the items have a gradient of 0. In order to achieve this, we create a matrix with full zeros in the same size as logprops and only set the gradient for those 27 letter predictions which we use in the loss calculation."],"metadata":{"id":"BiMTbrDnnX3w"}},{"cell_type":"code","source":["dlogprobs = torch.zeros_like(logprobs)\n","dlogprobs[range(n), Yb] = -1.0/n\n","cmp('logprobs', dlogprobs, logprobs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pidfpN6epNza","executionInfo":{"status":"ok","timestamp":1703449574421,"user_tz":300,"elapsed":215,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"4898f08d-707d-4bb3-9cbf-20aab2ad6289"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `dprobs`\n","\n"],"metadata":{"id":"JMb8qA0vpxfD"}},{"cell_type":"markdown","source":["The use of probs in our forward function goes as follows:\n","\n","```python\n","logprobs = probs.log()\n","```\n","\n","What is the derivative of the log function?\n","\n","$\\frac{d}{dx}log_a(x) = \\frac{1}{xln(a)}$\n","\n","With the `.log()` function, $a = e$, meaning the derivative is just $\\frac{1}{x}$. So our caculation of probs should be fairly straight forward, **but don't forget the chain rule**. We must multiple by the derivative of logprobs as well."],"metadata":{"id":"pbRQ-Y2Vwgv1"}},{"cell_type":"code","source":["dprobs = dlogprobs * (1.0 / probs)\n","cmp('probs', dprobs, probs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiwxYWaBtXBU","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"1978f7c9-f3d8-47cc-99b8-a561825bd03e"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `dcounts` & `dcounts_sum_inv`"],"metadata":{"id":"ZiQAnIDbwkEX"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","probs = counts * counts_sum_inv\n","```\n","\n","`counts` and `counts_sum_inv`, at first glance, are pretty intuitive calculation, but there's a catch. While we can calculate `counts_sum_inv` with only the chain rule, `counts` is used else where in the code. As in micrograd, we'll sum the gradients to find `dcounts`, but until then, we don't have a final value for `dcounts`.\n","\n","For finding `dcounts_sum_inv`, we must look at the shapes involved in our operations."],"metadata":{"id":"jSalsrfIwydG"}},{"cell_type":"code","source":["counts.shape, counts_sum_inv.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4-2TaOhycEO","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"293be51f-22f3-447d-f9f7-afba8a6bea38"},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([32, 27]), torch.Size([32, 1]))"]},"metadata":{},"execution_count":78}]},{"cell_type":"markdown","source":["There is some broadcasting going on with our element-wise multiplication between `counts` and `counts_sum_inv`. This broadcasting is happening along the second dimension, is a single column, duplicated to fill 27 columns for each row.\n","\n","In doing our manual back propagation, we need to sum along this dimension."],"metadata":{"id":"avOJe09xyjIS"}},{"cell_type":"code","source":["dcounts = dprobs * counts_sum_inv # Not final\n","dcounts_sum_inv = (dprobs * counts).sum(1, keepdim = True)\n","dcounts_sum_inv = dcounts_sum_inv.sum(1, keepdim = True)\n","cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6-n6FyHxLZ3","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"bf700395-63b5-483d-9543-2c3e79dac206"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `dcounts_sum`"],"metadata":{"id":"9S8WOArPztlM"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","counts_sum_inv = counts_sum**-1\n","```\n","\n","This should be an easy use of the chain rule, but we also need to know the derivative of $\\frac{1}{x}$, which is: $\\frac{-1}{x^2}$.\n","\n","To be sure from now on of broadcasting errors, let's print the shapes of our tensors."],"metadata":{"id":"T--o6lHizvV1"}},{"cell_type":"code","source":["counts_sum.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KF3094c91Isy","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"cde8c6af-4f25-41d5-e30e-15f4ed822ef5"},"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 1])"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["dcounts_sum = dcounts_sum_inv * -(counts_sum**-2)\n","cmp('dcounts_sum', dcounts_sum, counts_sum)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"absmDoYP1LSM","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"4e3921a2-3457-4cb7-9258-912c4c5ac102"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["dcounts_sum     | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `dcounts`"],"metadata":{"id":"zilJaFQg1k-D"}},{"cell_type":"markdown","source":["`dcounts` has already been partially calculated since it was used with `dcounts_sum_inv`. Now, we'll complete this calculation of `dcounts` and add onto the previous result.\n","\n","**Code:**\n","```python\n","counts_sum = counts.sum(1, keepdim = True)\n","```\n","\n","In this case, we're summing across the second dimension. Two steps ago, in reflection of some broadcasting across a certain, we summed across that dimension to get the derivative. In the same mindset, when we sum across the second dimension, we're going to broadcast `dcounts_sum` into the dimensions of `counts`."],"metadata":{"id":"EQaRKZ8F1va8"}},{"cell_type":"code","source":["counts.shape, counts_sum.shape, dcounts_sum.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUBBr_Ui2oAR","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"fe7a7955-20fe-4059-9895-412ab7db2f37"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([32, 27]), torch.Size([32, 1]), torch.Size([32, 1]))"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["dcounts += dcounts_sum.broadcast_to((32, 27))\n","cmp('dcounts', dcounts, counts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSqbfV7i2xIj","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"e7958558-afee-442e-d5f1-b5b43ca2b794"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `dnorm_logits`"],"metadata":{"id":"bm9PAWb977fg"}},{"cell_type":"markdown","source":["For `dnorm_logits`, the calculation should be fairly simple. First, we need the derivative of $e^x$, which is fortunately $e^x$. This means that `norm_logits.exp()` is our local derivative and all we have to do is multiply by our outer derivative, `dcounts`.\n","\n","**Code:**\n","```python\n","counts = norm_logits.exp()\n","```"],"metadata":{"id":"rtwmYJvB8DJC"}},{"cell_type":"code","source":["counts.shape, norm_logits.shape, dcounts.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_bSpquT8gyx","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"fc20c04f-f2fe-4f5d-dc81-c8553706d314"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 27]))"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["dnorm_logits = dcounts * norm_logits.exp()\n","cmp('dnorm_logits', dnorm_logits, norm_logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWbk32YA8j5X","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"fda05354-4bcd-4d28-db10-bf6f5fffa7e4"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `dlogits` & `dlogit_maxes`"],"metadata":{"id":"hq6AvP7d9YKB"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","logit_maxes = logits.max(1, keepdim=True).values\n","norm_logits = logits - logit_maxes\n","```\n","\n","Let's start doing bigger pieces. We need the derivative of `logits` and `logit_maxes`, which are used in these two lines. Let's start with `logit_maxes` since it is only used in calculating `norm_logits`.\n","\n","We know from micrograd that when there is an addition (or subtraction), the outer derivative pushed onto the local derivatives. So, `dlogit_maxes` is just the negation of `dnorm_logits`."],"metadata":{"id":"LcLq07um9kSC"}},{"cell_type":"code","source":["dnorm_logits.shape, logits.shape, logit_maxes.shape # Some broadcasting on logit_maxes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jNGFgjvL-0EK","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"5266b3bd-c88f-489e-ed4f-cb7a630eb334"},"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"]},"metadata":{},"execution_count":86}]},{"cell_type":"markdown","source":["So turns out there's some broadcasting being done on `logit_maxes`, specifically on replicating single column into 27 columns for each row. In order to reverse this, we need to do the steps as previously mentioned to get the `dlogit_maxes`, but the sum across that second dimension."],"metadata":{"id":"2VyPr51I-9lN"}},{"cell_type":"code","source":["dlogit_maxes = (-dnorm_logits).sum(1, keepdim = True)\n","cmp('dlogit_maxes', dlogit_maxes, logit_maxes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fhg9XrP-_aLT","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"6120b0af-0a24-4427-cc10-650ccfdf54c3"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["Now onto `dlogits`. When we first calculated `dlogprobs`, we indexed the array for only predictions of the actual next character. This indexing meant that the gradients for predictions of characters which were not used were set to 0. Something very similar should be done in calculating `dlogits`. Finding the max of each row means all the other values which aren't the max end with a gradient of 0.\n","\n","Our job is to now set the gradient for all of the items to be 0, then fill in the values of our maxes in their respected positions."],"metadata":{"id":"HWLehBofAFDM"}},{"cell_type":"code","source":["logit_maxes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FA9djJjKC4AW","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"05b9dc0a-f961-4e75-ac50-743409612814"},"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.1046],\n","        [0.9205],\n","        [1.1279],\n","        [0.7170],\n","        [1.7313],\n","        [0.8735],\n","        [0.8433],\n","        [1.3931],\n","        [1.0201],\n","        [1.0004],\n","        [1.7624],\n","        [2.0121],\n","        [1.0004],\n","        [0.7677],\n","        [0.4990],\n","        [0.8161],\n","        [0.9938],\n","        [0.8280],\n","        [1.0004],\n","        [0.8308],\n","        [0.8102],\n","        [1.0501],\n","        [1.0004],\n","        [1.1752],\n","        [1.5557],\n","        [1.0327],\n","        [1.1393],\n","        [0.8972],\n","        [1.0149],\n","        [0.7909],\n","        [1.1229],\n","        [0.9439]], grad_fn=<MaxBackward0>)"]},"metadata":{},"execution_count":88}]},{"cell_type":"code","source":["# dlogits = dnorm_logits.clone()\n","# max_values = torch.zeros_like(logits)\n","\n","# row_i = 0\n","# max_index_pair = zip(logits.max(dim = 1)[0], logits.max(dim = 1)[1])\n","# for value, index in max_index_pair:\n","#   max_values[row_i][index] = value\n","#   row_i += 1\n","\n","# dlogits += max_values * dlogit_maxes\n","# cmp('dlogits [MINE]', dlogits, logits)\n","\n","# Karpathy's code:\n","dlogits = dnorm_logits.clone()\n","dlogits += F.one_hot(logits.max(1).indices,\n","                     num_classes = logits.shape[1]) * dlogit_maxes\n","cmp('dlogits [KARP]', dlogits, logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CfJrcr_gB5nF","executionInfo":{"status":"ok","timestamp":1703449574422,"user_tz":300,"elapsed":2,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"9f1067c6-31d7-4b25-deba-95d8ef290470"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["dlogits [KARP]  | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `dH`, `dW`, & `db`"],"metadata":{"id":"RZdTE7_kZiZL"}},{"cell_type":"markdown","source":["To calculate the partial derivatives of `H`, `W`, and `b`, we need to know how to do the derivative of matrix multiplications. If we multiply the outer derivative by the transpose of the multiplication which multiples, for example `h`, we can find `dh`.\n","\n","**Code:**\n","```python\n","logits = h @ W2 + b2 # output layer\n","```"],"metadata":{"id":"pPvChY8fZyxW"}},{"cell_type":"code","source":["print(f\"dlogits: {dlogits.shape}\")\n","print(f\"h: {h.shape}\")\n","print(f\"W2: {W2.shape}\")\n","print(f\"b2: {b2.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFMomlnWdYiI","executionInfo":{"status":"ok","timestamp":1703449574631,"user_tz":300,"elapsed":211,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"e15b50f0-bc74-44ec-90c1-3dd1ed07b135"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["dlogits: torch.Size([32, 27])\n","h: torch.Size([32, 64])\n","W2: torch.Size([64, 27])\n","b2: torch.Size([27])\n"]}]},{"cell_type":"markdown","source":["How do we know which matrices to multiply and which matrices to transpose? Our `dh` needs to have the same shape as `h`. `h.shape` is (32, 64), so we need to multiply to matrices that result in a matrix what is also (32, 64). In this case, it's `dlogits` and `W2.T`."],"metadata":{"id":"yahOsZlAeJbi"}},{"cell_type":"code","source":["dh = dlogits @ W2.T\n","dW2 = h.T @ dlogits\n","cmp('dh', dh, h)\n","cmp('dW2', dW2, W2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uj_DHaTuZqe4","executionInfo":{"status":"ok","timestamp":1703449574631,"user_tz":300,"elapsed":7,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"76e0ebf8-ddd2-4bc8-d380-f1b8bbf8a905"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["dh              | exact: True  | approximate: True  | maxdiff: 0.0\n","dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["Now for calculating `db2`, we need to pay attention to the dimensions of `b2`. There's some broadcasting going on, but in the 0th dimension. So, in order to get back to these dimensions, we need to sum across the first dimension of `dlogits`. We don't need `keepdim` to be True, since we want (27), not (1, 27)."],"metadata":{"id":"CQ0o2mQ_frkO"}},{"cell_type":"code","source":["db2 = dlogits.sum(0)\n","cmp('db2', db2, b2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRSeDDYRgcMY","executionInfo":{"status":"ok","timestamp":1703449574631,"user_tz":300,"elapsed":7,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"b8dd865d-e0bf-4e61-f8ac-6c0a2068709b"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["db2             | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `dhpreact`"],"metadata":{"id":"tBROX2rbiVWE"}},{"cell_type":"markdown","source":["For `dhpreact`, we need to know the derivative of tanh and use the product rule. We can look up the derivative, which is: $1 - tanh^2(x)$. Also, we don't need to calculate of $tanh(preact)$, since that already equals to h.\n","\n","**Code:**\n","```python\n","h = torch.tanh(hpreact)\n","```"],"metadata":{"id":"eCkS48suibAS"}},{"cell_type":"code","source":["h.shape, dh.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1SV6be8LjR1N","executionInfo":{"status":"ok","timestamp":1703449574631,"user_tz":300,"elapsed":7,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"a6dd8cd1-5ee7-4fe8-e6ee-c1096c0a43d8"},"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([32, 64]), torch.Size([32, 64]))"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["dhpreact = (1.0 - h**2) * dh\n","cmp('dhpreact', dh, h)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"laRl90_ziaYx","executionInfo":{"status":"ok","timestamp":1703449574631,"user_tz":300,"elapsed":6,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"c31d0dab-45a8-4587-fb25-99a7fc98a8c2"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["dhpreact        | exact: True  | approximate: True  | maxdiff: 0.0\n"]}]},{"cell_type":"markdown","source":["### `bngain`, `bnraw`, & `bnbias`"],"metadata":{"id":"mFteK1e6jvZX"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","hpreact = bngain * bnraw + bnbias\n","```"],"metadata":{"id":"EjPVpCYBj7NW"}},{"cell_type":"code","source":["print(f\"dhpreact: {dhpreact.shape}\")\n","print(f\"bngain: {bngain.shape}\")\n","print(f\"bnraw: {bnraw.shape}\")\n","print(f\"bnbias: {bnbias.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2V5KNVNkERj","executionInfo":{"status":"ok","timestamp":1703449574632,"user_tz":300,"elapsed":7,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"a86b81e8-6b41-4376-8be3-55586f4567e7"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["dhpreact: torch.Size([32, 64])\n","bngain: torch.Size([1, 64])\n","bnraw: torch.Size([32, 64])\n","bnbias: torch.Size([1, 64])\n"]}]},{"cell_type":"code","source":["dbngain = (bnraw * dhpreact).sum(0, keepdim = True)\n","dbnraw = bngain.broadcast_to((32, 64)) * dhpreact\n","dbnbias = dhpreact.sum(0, keepdim = True)\n","\n","cmp('dbngain', dbngain, bngain)\n","cmp('dbnraw', dbnraw, bnraw)\n","cmp('dbnbias', dbnbias, bnbias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-x0QN9tkhqQ","executionInfo":{"status":"ok","timestamp":1703449574632,"user_tz":300,"elapsed":6,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"5d882316-aabc-4dee-8bbc-f60f96f02775"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["dbngain         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","dbnraw          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","dbnbias         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"]}]},{"cell_type":"markdown","source":["### `dbnvar_inv` & `dbndiff`"],"metadata":{"id":"CKGmxeNt-cBl"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","bnraw = bndiff * bnvar_inv\n","```\n","\n","Calculating `dbnvar_inv` is fairely intuitive, since we have the outside derivative and can apply the chain rule. However, calculating `dbndiff`requires the gradients from the other operations, so we must finish the rest of the steps."],"metadata":{"id":"gdyPO5E7-igX"}},{"cell_type":"code","source":["bnraw.shape, bndiff.shape, bnvar_inv.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6j1PLb6_CfO","executionInfo":{"status":"ok","timestamp":1703449574632,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"a8d7c7a1-e06e-4d1a-d5ad-bbfb47f625dc"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"]},"metadata":{},"execution_count":97}]},{"cell_type":"markdown","source":["There's some implict broadcasting, so we'll have to sum across the first dimension."],"metadata":{"id":"7nsKTmrx_PTq"}},{"cell_type":"code","source":["dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim = True)\n","dbndiff =(dbnraw * bnvar_inv)\n","cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m0T7lLa7_aIP","executionInfo":{"status":"ok","timestamp":1703449574632,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"30cb9b4f-f15f-4286-e1f3-53cb9087ad64"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["dbnvar_inv      | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"]}]},{"cell_type":"markdown","source":["Let's come back to `dbndiff` as we go through each other line."],"metadata":{"id":"v25yi6y2AB65"}},{"cell_type":"markdown","source":["### `dbnvar`\n","\n"],"metadata":{"id":"Ec3KCuLzAb38"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","bnvar_inv = (bnvar + 1e-5)**-0.5\n","```\n","\n","Let's map out the equation more clearly with latex:\n","\n","$y=\\frac{1}{\\sqrt{x+ 10^{-5}}}$\n","\n","We can use the exponent rule to get the derivative."],"metadata":{"id":"Ad1AuUEaCz7j"}},{"cell_type":"code","source":["bnvar_inv.shape, bnvar.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llBMi2TFABmI","executionInfo":{"status":"ok","timestamp":1703449574632,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"471ac1c0-7f70-4b04-d374-cb5805dca2c3"},"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 64]), torch.Size([1, 64]))"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["dbnvar = dbnvar_inv * (-0.5 * (bnvar + 1e-5)**-1.5)\n","cmp('dbnvar', dbnvar, bnvar)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6r1ShqtXAw8v","executionInfo":{"status":"ok","timestamp":1703449574632,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"25325d8f-acd7-48ac-f431-60dd399d97cf"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["dbnvar          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n"]}]},{"cell_type":"markdown","source":["### `dbndiff2` & `dbndiff`"],"metadata":{"id":"ByK04yggUd7n"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n","bndiff2 = bndiff**2\n","```\n","\n","In latex:\n","$\\frac{1}{n-1}*{x_{summed}}$"],"metadata":{"id":"tP-6FdxeUm0Z"}},{"cell_type":"code","source":["bnvar.shape, bndiff2.shape, dbnvar.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-Q-e3YCUmi7","executionInfo":{"status":"ok","timestamp":1703449574632,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"d00f2a41-c494-451c-892a-77ab411cb282"},"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["dbndiff2 = 1.0 / (n-1.0) * dbnvar.broadcast_to((32, 64))\n","cmp('dbndiff2', dbndiff2, bndiff2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uObma8rXU12c","executionInfo":{"status":"ok","timestamp":1703449605414,"user_tz":300,"elapsed":137,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"0caac1bc-7698-458e-958c-62ff78d66162"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["dbndiff2        | exact: False | approximate: True  | maxdiff: 2.1827872842550278e-11\n"]}]},{"cell_type":"markdown","source":["Now onto `dbndiff`. Remember, we calculated the first part of dbndiff two examples ago, so we'll have to sum this next calculation onto the existing dbndiff. For reference, it was calculated as follows:\n","\n","```python\n","dbndiff =(dbnraw * bnvar_inv)\n","```"],"metadata":{"id":"8ZM_BFCjW4_Q"}},{"cell_type":"code","source":["dbndiff = (dbnraw * dbnvar_inv)\n","dbndiff += (2 * bndiff) * dbndiff2\n","cmp('dbndiff', dbndiff, bndiff)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoxSHh9xW4l-","executionInfo":{"status":"ok","timestamp":1703449609560,"user_tz":300,"elapsed":132,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"87ef40e0-7ffb-4253-9592-1cda2b9e9408"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["dbndiff         | exact: False | approximate: False | maxdiff: 0.007316019851714373\n"]}]},{"cell_type":"markdown","source":["### `dhprebn` & `bnmeani`"],"metadata":{"id":"EPUMAR8kXoq8"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","bndiff = hprebn - bnmeani\n","```"],"metadata":{"id":"RzJwqTb4YM7U"}},{"cell_type":"code","source":["bndiff.shape, hprebn.shape, bnmeani.shape # Some broadcasting on bnmeani"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_WoFNKeYqpx","executionInfo":{"status":"ok","timestamp":1703449574812,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"6b232204-6290-403c-a9c0-896d7f668058"},"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","source":["dbnmeani = -dbndiff.sum(0, keepdim = True)\n","cmp('dbnmeani', dbnmeani, bnmeani)\n","\n","dhprebn = dbndiff\n","dhprebn += 1/n * dbnmeani.broadcast_to((32, 64))\n","cmp('dhprebn', dhprebn, hprebn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Pn6XIx2Yye3","executionInfo":{"status":"ok","timestamp":1703449574812,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"ca97f043-09e0-4a53-9b3a-58f3d07284fe"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["dbnmeani        | exact: False | approximate: False | maxdiff: 0.03485224395990372\n","dhprebn         | exact: False | approximate: False | maxdiff: 0.00754727004095912\n"]}]},{"cell_type":"markdown","source":["### `dembcat`, `W1`, & `b1`"],"metadata":{"id":"M-G_f7Hlah-c"}},{"cell_type":"markdown","source":["**Code:**\n","```python\n","hprebn = embcat @ W1 + b1\n","```"],"metadata":{"id":"3hpLzd7oaksr"}},{"cell_type":"code","source":["embcat.shape, W1.shape, b1.shape, dhprebn.shape # Some broadcasting on b1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhrWi7V1aqvn","executionInfo":{"status":"ok","timestamp":1703449574812,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"60973abd-4a18-47e9-846e-2209017bdb2a"},"execution_count":106,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([32, 30]),\n"," torch.Size([30, 64]),\n"," torch.Size([64]),\n"," torch.Size([32, 64]))"]},"metadata":{},"execution_count":106}]},{"cell_type":"code","source":["dembcat = dhprebn @ W1.T\n","dW1 = embcat.T @ dhprebn\n","db1 = dhprebn.sum(0)\n","\n","cmp('dembcat', dembcat, embcat)\n","cmp('dW1', dW1, W1)\n","cmp('db1', db1, b1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NN-jhFX3ayzf","executionInfo":{"status":"ok","timestamp":1703449574812,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"0492a28b-614d-415e-b221-d9f2f76e7298"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["dembcat         | exact: False | approximate: False | maxdiff: 0.012269515544176102\n","dW1             | exact: False | approximate: False | maxdiff: 0.03635820001363754\n","db1             | exact: False | approximate: True  | maxdiff: 2.2992026060819626e-09\n"]}]},{"cell_type":"markdown","source":["### `demb` & `dC`"],"metadata":{"id":"osOcV3VwfAAF"}},{"cell_type":"markdown","source":["**Code**:\n","```python\n","emb = C[Xb]\n","embcat = emb.view(emb.shape[0], -1)\n","```"],"metadata":{"id":"LzwJj1lXfB33"}},{"cell_type":"code","source":["demb = dembcat.view(emb.shape)\n","dC = torch.zeros_like(C)\n","for k in range(Xb.shape[0]):\n","  for j in range(Xb.shape[1]):\n","    ix = Xb[k, j]\n","    dC[ix] += demb[k, j]\n","\n","cmp('emb', demb, emb)\n","cmp('C', dC, C)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3gDD_ACfBoT","executionInfo":{"status":"ok","timestamp":1703449574812,"user_tz":300,"elapsed":4,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"f9930f7d-3607-4b46-881d-be7ce75910b7"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["emb             | exact: False | approximate: False | maxdiff: 0.012269515544176102\n","C               | exact: False | approximate: False | maxdiff: 0.04075043275952339\n"]}]},{"cell_type":"markdown","source":["## Testing Our Backward Pass"],"metadata":{"id":"e_Nia0gDzrAK"}},{"cell_type":"code","execution_count":109,"metadata":{"id":"mO-8aqxK8PPw","executionInfo":{"status":"ok","timestamp":1703449574812,"user_tz":300,"elapsed":3,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"503055ef-273e-4eb6-9843-fea17f38e566"},"outputs":[{"output_type":"stream","name":"stdout","text":["counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n","counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n","counts          | exact: True  | approximate: True  | maxdiff: 0.0\n","norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n","logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n","logits          | exact: True  | approximate: True  | maxdiff: 0.0\n","h               | exact: True  | approximate: True  | maxdiff: 0.0\n","W2              | exact: True  | approximate: True  | maxdiff: 0.0\n","b2              | exact: True  | approximate: True  | maxdiff: 0.0\n","hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n","bnraw           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n","bnvar           | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n","bndiff2         | exact: False | approximate: True  | maxdiff: 2.1827872842550278e-11\n","bndiff          | exact: False | approximate: False | maxdiff: 0.007315183524042368\n","bnmeani         | exact: False | approximate: False | maxdiff: 0.03485224395990372\n","hprebn          | exact: False | approximate: False | maxdiff: 0.00754727004095912\n","embcat          | exact: False | approximate: False | maxdiff: 0.012269515544176102\n","W1              | exact: False | approximate: False | maxdiff: 0.03635820001363754\n","b1              | exact: False | approximate: True  | maxdiff: 2.2992026060819626e-09\n","emb             | exact: False | approximate: False | maxdiff: 0.012269515544176102\n","C               | exact: False | approximate: False | maxdiff: 0.04075043275952339\n"]}],"source":["# Exercise 1: backprop through the whole thing manually,\n","# backpropagating through exactly all of the variables\n","# as they are defined in the forward pass above, one by one\n","\n","cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n","cmp('counts_sum', dcounts_sum, counts_sum)\n","cmp('counts', dcounts, counts)\n","cmp('norm_logits', dnorm_logits, norm_logits)\n","cmp('logit_maxes', dlogit_maxes, logit_maxes)\n","cmp('logits', dlogits, logits)\n","cmp('h', dh, h)\n","cmp('W2', dW2, W2)\n","cmp('b2', db2, b2)\n","cmp('hpreact', dhpreact, hpreact)\n","cmp('bngain', dbngain, bngain)\n","cmp('bnbias', dbnbias, bnbias)\n","cmp('bnraw', dbnraw, bnraw)\n","cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n","cmp('bnvar', dbnvar, bnvar)\n","cmp('bndiff2', dbndiff2, bndiff2)\n","cmp('bndiff', dbndiff, bndiff)\n","cmp('bnmeani', dbnmeani, bnmeani)\n","cmp('hprebn', dhprebn, hprebn)\n","cmp('embcat', dembcat, embcat)\n","cmp('W1', dW1, W1)\n","cmp('b1', db1, b1)\n","cmp('emb', demb, emb)\n","cmp('C', dC, C)"]},{"cell_type":"markdown","source":["## Excercise 2"],"metadata":{"id":"XQFXxxP0hEZI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebLtYji_8PPw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703384195892,"user_tz":300,"elapsed":6,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"a3355e31-b6f1-49a7-e751-fd9f1a5bc470"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.3346469402313232 diff: 0.0\n"]}],"source":["# Exercise 2: backprop through cross_entropy but all in one go\n","# to complete this challenge look at the mathematical expression of the loss,\n","# take the derivative, simplify the expression, and just write it out\n","\n","# forward pass\n","\n","# before:\n","# logit_maxes = logits.max(1, keepdim=True).values\n","# norm_logits = logits - logit_maxes # subtract max for numerical stability\n","# counts = norm_logits.exp()\n","# counts_sum = counts.sum(1, keepdims=True)\n","# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","# probs = counts * counts_sum_inv\n","# logprobs = probs.log()\n","# loss = -logprobs[range(n), Yb].mean()\n","\n","# now:\n","loss_fast = F.cross_entropy(logits, Yb)\n","print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"]},{"cell_type":"markdown","source":["With calculating the partial derivative of the cross entropy function with respect to the loss, we do all the math and it eventually canceles to $P_i$ or $P_i - 1$, where $P$ is the prediction vector."],"metadata":{"id":"Pj6n_k2Hhrk-"}},{"cell_type":"code","execution_count":112,"metadata":{"id":"-gCXbB4C8PPx","executionInfo":{"status":"ok","timestamp":1703450698875,"user_tz":300,"elapsed":133,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a808e6cb-e273-4558-e69f-acbbad5415e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["logits          | exact: False | approximate: True  | maxdiff: 7.683411240577698e-09\n"]}],"source":["# backward pass\n","dlogits = F.softmax(logits, 1)\n","dlogits[range(n), Yb] -= 1\n","dlogits /= n\n","\n","cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"]},{"cell_type":"code","source":["F.softmax(logits, 1)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79V0xgL6lXJR","executionInfo":{"status":"ok","timestamp":1703450892224,"user_tz":300,"elapsed":146,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"45b0ab11-9ebc-4b96-b7d0-5b9c36aa19cf"},"execution_count":119,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0737, 0.0957, 0.0215, 0.0454, 0.0201, 0.0780, 0.0227, 0.0323, 0.0180,\n","        0.0291, 0.0357, 0.0356, 0.0347, 0.0294, 0.0364, 0.0152, 0.0099, 0.0190,\n","        0.0160, 0.0513, 0.0489, 0.0226, 0.0233, 0.0762, 0.0609, 0.0261, 0.0225],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":119}]},{"cell_type":"code","source":["dlogits[0] * n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"paPa2Ha4k1gH","executionInfo":{"status":"ok","timestamp":1703450735013,"user_tz":300,"elapsed":152,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"fa55b04a-6c98-43a9-835c-9119d1a4a94b"},"execution_count":113,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.0737,  0.0957,  0.0215,  0.0454,  0.0201,  0.0780,  0.0227,  0.0323,\n","        -0.9820,  0.0291,  0.0357,  0.0356,  0.0347,  0.0294,  0.0364,  0.0152,\n","         0.0099,  0.0190,  0.0160,  0.0513,  0.0489,  0.0226,  0.0233,  0.0762,\n","         0.0609,  0.0261,  0.0225], grad_fn=<MulBackward0>)"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["dlogits[0].sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbPfmTpHlFqg","executionInfo":{"status":"ok","timestamp":1703450801358,"user_tz":300,"elapsed":178,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"7a0f4d86-857b-4a9c-d839-85578025e388"},"execution_count":116,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-2.5611e-09, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":116}]},{"cell_type":"code","source":["plt.figure(figsize =(8, 8))\n","plt.imshow(dlogits.detach(), cmap = 'gray')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":694},"id":"liJ2_idik3rK","executionInfo":{"status":"ok","timestamp":1703450770642,"user_tz":300,"elapsed":1004,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"fc96dff4-f527-4551-ea84-b173b1f93660"},"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7a1d4c3c8340>"]},"metadata":{},"execution_count":115},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x800 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["Here with `dlogits`, we're measuring how much we need to sway the whole neural network based on it's output. In this specific example, the black squares are the predictions for the correct next character after a subtraction of one. If the model is completely accurate, than it would be 1 minus 1. The rest of the predictions would also be 0 in this case.\n","\n","Why is this important? It shows us how gradient descent is trying to move the weights of the model. \"The amount of which you mis-predict, is the amount of the strength of the pull\"."],"metadata":{"id":"eHQIQGuTlls2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hd-MkhB68PPy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703384195892,"user_tz":300,"elapsed":5,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"0ffc7d13-7bd7-467b-ac1e-08e45987de59"},"outputs":[{"output_type":"stream","name":"stdout","text":["max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"]}],"source":["# Exercise 3: backprop through batchnorm but all in one go\n","# to complete this challenge look at the mathematical expression of the output of batchnorm,\n","# take the derivative w.r.t. its input, simplify the expression, and just write it out\n","# BatchNorm paper: https://arxiv.org/abs/1502.03167\n","\n","# forward pass\n","\n","# before:\n","# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","# bndiff = hprebn - bnmeani\n","# bndiff2 = bndiff**2\n","# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","# bnvar_inv = (bnvar + 1e-5)**-0.5\n","# bnraw = bndiff * bnvar_inv\n","# hpreact = bngain * bnraw + bnbias\n","\n","# now:\n","hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n","print('max diff:', (hpreact_fast - hpreact).abs().max())"]},{"cell_type":"code","execution_count":120,"metadata":{"id":"POdeZSKT8PPy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703452076984,"user_tz":300,"elapsed":156,"user":{"displayName":"David Ulloa","userId":"00180249129585986739"}},"outputId":"974aa5f6-521a-4976-ddb5-adeffbe015db"},"outputs":[{"output_type":"stream","name":"stdout","text":["hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"]}],"source":["# backward pass\n","\n","# before we had:\n","# dbnraw = bngain * dhpreact\n","# dbndiff = bnvar_inv * dbnraw\n","# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n","# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n","# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n","# dbndiff += (2*bndiff) * dbndiff2\n","# dhprebn = dbndiff.clone()\n","# dbnmeani = (-dbndiff).sum(0)\n","# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n","\n","# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n","# (you'll also need to use some of the variables from the forward pass up above)\n","\n","# -----------------\n","# YOUR CODE HERE :)\n","dhprebn = bngain*bnvar_inv/n * (n * dhpreact - dhpreact.sum(0) - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0))\n","# -----------------\n","\n","cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPy8DhqB8PPz"},"outputs":[],"source":["# Exercise 4: putting it all together!\n","# Train the MLP neural net with your own backward pass\n","\n","# init\n","n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True\n","\n","# same optimization as last time\n","max_steps = 200000\n","batch_size = 32\n","n = batch_size # convenience\n","lossi = []\n","\n","# use this context manager for efficiency once your backward pass is written (TODO)\n","#with torch.no_grad():\n","\n","# kick off optimization\n","for i in range(max_steps):\n","\n","  # minibatch construct\n","  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n","\n","  # forward pass\n","  emb = C[Xb] # embed the characters into vectors\n","  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","  # Linear layer\n","  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n","  # BatchNorm layer\n","  # -------------------------------------------------------------\n","  bnmean = hprebn.mean(0, keepdim=True)\n","  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n","  bnvar_inv = (bnvar + 1e-5)**-0.5\n","  bnraw = (hprebn - bnmean) * bnvar_inv\n","  hpreact = bngain * bnraw + bnbias\n","  # -------------------------------------------------------------\n","  # Non-linearity\n","  h = torch.tanh(hpreact) # hidden layer\n","  logits = h @ W2 + b2 # output layer\n","  loss = F.cross_entropy(logits, Yb) # loss function\n","\n","  # backward pass\n","  for p in parameters:\n","    p.grad = None\n","  loss.backward() # use this for correctness comparisons, delete it later!\n","\n","  # manual backprop! #swole_doge_meme\n","  # -----------------\n","  # YOUR CODE HERE :)\n","  dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n","  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n","  # -----------------\n","\n","  # update\n","  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n","  for p, grad in zip(parameters, grads):\n","    p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n","    #p.data += -lr * grad # new way of swole doge TODO: enable\n","\n","  # track stats\n","  if i % 10000 == 0: # print every once in a while\n","    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","  lossi.append(loss.log10().item())\n","\n","  if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEpI0hMW8PPz"},"outputs":[],"source":["# useful for checking your gradients\n","# for p,g in zip(parameters, grads):\n","#   cmp(str(tuple(p.shape)), g, p)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KImLWNoh8PP0"},"outputs":[],"source":["# calibrate the batch norm at the end of training\n","\n","with torch.no_grad():\n","  # pass the training set through\n","  emb = C[Xtr]\n","  embcat = emb.view(emb.shape[0], -1)\n","  hpreact = embcat @ W1 + b1\n","  # measure the mean/std over the entire training set\n","  bnmean = hpreact.mean(0, keepdim=True)\n","  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aFnP_Zc8PP0"},"outputs":[],"source":["# evaluate train and val loss\n","\n","@torch.no_grad() # this decorator disables gradient tracking\n","def split_loss(split):\n","  x,y = {\n","    'train': (Xtr, Ytr),\n","    'val': (Xdev, Ydev),\n","    'test': (Xte, Yte),\n","  }[split]\n","  emb = C[x] # (N, block_size, n_embd)\n","  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","  hpreact = embcat @ W1 + b1\n","  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","  h = torch.tanh(hpreact) # (N, n_hidden)\n","  logits = h @ W2 + b2 # (N, vocab_size)\n","  loss = F.cross_entropy(logits, y)\n","  print(split, loss.item())\n","\n","split_loss('train')\n","split_loss('val')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esWqmhyj8PP1"},"outputs":[],"source":["# I achieved:\n","# train 2.0718822479248047\n","# val 2.1162495613098145"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHeQNv3s8PP1"},"outputs":[],"source":["# sample from the model\n","g = torch.Generator().manual_seed(2147483647 + 10)\n","\n","for _ in range(20):\n","\n","    out = []\n","    context = [0] * block_size # initialize with all ...\n","    while True:\n","      # forward pass\n","      emb = C[torch.tensor([context])] # (1,block_size,d)\n","      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","      hpreact = embcat @ W1 + b1\n","      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","      h = torch.tanh(hpreact) # (N, n_hidden)\n","      logits = h @ W2 + b2 # (N, vocab_size)\n","      # sample\n","      probs = F.softmax(logits, dim=1)\n","      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n","      context = context[1:] + [ix]\n","      out.append(ix)\n","      if ix == 0:\n","        break\n","\n","    print(''.join(itos[i] for i in out))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-","timestamp":1702930918604}]}},"nbformat":4,"nbformat_minor":0}